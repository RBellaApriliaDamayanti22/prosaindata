{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHu4Buqi2Lqr0yaxYKZHEN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## TUGAS 7"],"metadata":{"id":"0dHwqBxYILsY"}},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"Dz8lZyogcqXw"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vaX9s3E9Ic5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/prosaindata/"],"metadata":{"id":"ZPIAtIGiJXTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#install library\n","!pip install sastrawi\n","!pip install swifter\n","!pip install gensim"],"metadata":{"id":"rDCKMPjBS4Sh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import library\n","import pandas as pd\n","import numpy as np\n","from string import punctuation\n","import re\n","import nltk"],"metadata":{"id":"dGylIG-pc41O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load data\n","df = pd.read_excel(\"tugas/dataset/Data_TA.xlsx\")\n","df.head()"],"metadata":{"id":"-RJsxriCS63J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"Pse6efdPS9Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"G1Qa4C76TAO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna()"],"metadata":{"id":"6T60A5zDTClC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"IrwDQvR4TGF4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"Q0xyUeGHTIou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Label'].value_counts()"],"metadata":{"id":"wfdKO_ODTMp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing Data"],"metadata":{"id":"tf--s3A0TP7J"}},{"cell_type":"markdown","source":["#### 1. Symbol & Punctuation Removal, case folding"],"metadata":{"id":"Ed9vO2lwdSYr"}},{"cell_type":"code","source":["#proses menghilangkan simbol dan emoji\n","def remove_text_special (text):\n","  text = text.replace('\\\\t',\"\").replace('\\\\n',\"\").replace('\\\\u',\"\").replace('\\\\',\"\")\n","  text = text.encode('ascii', 'replace').decode('ascii')\n","  return text.replace(\"http://\",\" \").replace(\"https://\", \" \")\n","df['Abstrak'] = df['Abstrak'].apply(remove_text_special)\n","print(df['Abstrak'])"],"metadata":{"id":"x7V4d0c9TUOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_tanda_baca(text):\n","  text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n","  return text\n","\n","df['Abstrak'] = df['Abstrak'].apply(remove_tanda_baca)\n","df['Abstrak'].head(20)"],"metadata":{"id":"VkgtctcWTWim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#proses menghilangkan angka\n","def remove_numbers (text):\n","  return re.sub(r\"\\d+\", \"\", text)\n","df['Abstrak'] = df['Abstrak'].apply(remove_numbers)\n","df['Abstrak']"],"metadata":{"id":"fMNZZdxbTZvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#proses casefolding\n","def casefolding(Comment):\n","  Comment = Comment.lower()\n","  return Comment\n","df['Abstrak'] = df['Abstrak'].apply(casefolding)\n","df['Abstrak']"],"metadata":{"id":"JWqRa2tBTdCM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Word Normalization"],"metadata":{"id":"kd4Of0hQdjYA"}},{"cell_type":"code","source":["#proses tokenisasi\n","# from nltk.tokenize import TweetTokenizer\n","nltk.download('punkt')\n","# def word_tokenize(text):\n","#   tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","#   return tokenizer.tokenize(text)\n","\n","df['review_token'] = df['Abstrak'].apply(lambda sentence: nltk.word_tokenize(sentence))\n","df['review_token']"],"metadata":{"id":"cxxkcmmITfyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Normalisasi kata tidak baku\n","normalize = pd.read_excel(\"tugas/dataset/Normalization Data.xlsx\")\n","\n","normalize_word_dict = {}\n","\n","for row in normalize.iterrows():\n","  if row[0] not in normalize_word_dict:\n","    normalize_word_dict[row[0]] = row[1]\n","\n","def normalized_term(comment):\n","  return [normalize_word_dict[term] if term in normalize_word_dict else term for term in comment]\n","\n","df['comment_normalize'] = df['review_token'].apply(normalized_term)\n","df['comment_normalize'].head(20)"],"metadata":{"id":"HXHlZOaiPfU_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Tokenizing"],"metadata":{"id":"_zD8cB8mPn5n"}},{"cell_type":"markdown","source":["### 4. Stopwords Removal"],"metadata":{"id":"L0WoJ-wmPqb1"}},{"cell_type":"code","source":["#Stopword Removal\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","txt_stopwords = stopwords.words('indonesian')\n","\n","def stopwords_removal(filtering) :\n","  filtering = [word for word in filtering if word not in txt_stopwords]\n","  return filtering\n","\n","df['stopwords_removal'] = df['comment_normalize'].apply(stopwords_removal)\n","df['stopwords_removal'].head(20)"],"metadata":{"id":"_4fkZd3bPspP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#stopword removal 2\n","data_stopwords = pd.read_excel(\"tugas/dataset/list_stopwords.xlsx\")\n","print(data_stopwords)\n","\n","def stopwords_removal2(filter) :\n","  filter = [word for word in filter if word not in data_stopwords]\n","  return filter\n","\n","df['stopwords_removal_final'] = df['stopwords_removal'].apply(stopwords_removal2)\n","df['stopwords_removal_final'].head(20)"],"metadata":{"id":"s-R5vgK0Pu43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Stemming"],"metadata":{"id":"Z1KyMIT8PxtO"}},{"cell_type":"code","source":["#proses stem\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import string\n","import swifter\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","def stemming (term):\n","  return stemmer.stem(term)\n","\n","term_dict = {}\n","for document in df['stopwords_removal_final']:\n","  for term in document:\n","    if term not in term_dict:\n","      term_dict[term] = ''\n"],"metadata":{"id":"Se8ddqhWPzvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(term_dict))\n","print(\"-----------------------------\")"],"metadata":{"id":"yK6gncfjP129"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for term in term_dict:\n","  term_dict[term] = stemming(term)\n","  print(term,\":\",term_dict[term])\n","\n","print(term_dict)\n","print(\"-----------------------------\")"],"metadata":{"id":"qiSOqm7vP4BI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_stemming(document):\n","  return [term_dict[term] for term in document]"],"metadata":{"id":"cpeXXUiRP6kQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['stemming'] = df['stopwords_removal_final'].swifter.apply(get_stemming)"],"metadata":{"id":"EnGSw1NuP8pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df['stemming'])"],"metadata":{"id":"3bRL2soLP-j2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(20)"],"metadata":{"id":"oDFZh1X1QAR3"},"execution_count":null,"outputs":[]}]}