{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEiedyz4kRji2SOte4V9c/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0wNMcBkEVN7u"},"source":["## TUGAS 2 (Crawling data Website https://pta.trunojoyo.ac.id/ )"]},{"cell_type":"markdown","metadata":{"id":"aDx24WXHVV47"},"source":["### 1. Install modul Beautifulsoup dan request and Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AwIrfUbVdQN"},"outputs":[],"source":["import requests \n","from bs4 import BeautifulSoup \n","import csv\n","import pandas as pd\n","#agent user : supaya web server menampilkan konten yang dimiliki sesuai dengan OS yang kita gunakan, jadi web server tau browser dan OS yang kita gunakan\n","hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}"]},{"cell_type":"markdown","metadata":{"id":"OT70O_3TVmGp"},"source":["### Peroses crawling menggunakan bs4 dan request\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptB2gW0BVnMw"},"outputs":[],"source":["#Inisialisasi Link URL\n","url = 'https://pta.trunojoyo.ac.id/c_search/byprod/10/'\n","\n","headers={\n","    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n","    }\n","\n","#Inisialisai list untuk menginputkan hasil crawling kedalam sebuah list\n","listJudul = []\n","listPenulis = []\n","listAbstrak = []\n","\n","#proses perulangan untuk crawling data\n","for page in range(1,172):\n","  req = requests.get(url+str(page), headers=headers)\n","  soup = BeautifulSoup(req.text, 'html.parser')\n","  items = soup.findAll('li',{'data-id':'id-1'})\n","  for it in items:\n","    try: link = it.find('a', 'gray button')['href']\n","    except : link=''\n","    try: title = it.find('a', 'title').text \n","    except : title=''\n","    try: penulis = it.find('span').text.replace('Penulis :','') \n","    except : penulis=''\n","\n","    if it != '':\n","      listJudul.append(title)\n","      listPenulis.append(penulis)\n","      req2 = requests.get(str(link), headers=headers)\n","      soup2 = BeautifulSoup(req2.text, 'html.parser')\n","      items2 = soup2.findAll('li',{'data-id':'id-1'})\n","      for it2 in items2:\n","        try : abstrak = it2.find('p',{'align':'justify'}).text\n","        except : abstrak=''\n","\n","        if it2 != '':\n","          listAbstrak.append(abstrak)\n","  \n","judul = pd.DataFrame(listJudul,columns=[\"Judul\"])\n","penulis = pd.DataFrame(listPenulis,columns=[\"Penulis\"])\n","abstrak = pd.DataFrame(listAbstrak,columns=[\"Abstrak\"])\n","data = pd.concat([penulis,judul, abstrak], axis=1) \n","data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWUqdy7ERyNT"},"outputs":[],"source":["TA=scrape_TA(10)\n","TA_result = pd.DataFrame(TA)\n","TA_result.columns = [\"NPM\", \"Judul\",\"Abstrak\",\"Prodi\"]\n","TA_result"]},{"cell_type":"markdown","metadata":{"id":"scJbfmI5VtDJ"},"source":["### Export To CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1jGPwCfVwFF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/My Drive/prosaindata/CrawlingWebPTA.csv'\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  TA_result.to_csv(f)"]}]}