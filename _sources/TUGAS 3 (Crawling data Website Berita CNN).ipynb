{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3fOE3xmdFEtfUObVJUgxH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9jGA03wrWIpF"},"source":["## TUGAS 3 (Crawling data Website Berita CNN)\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eBjRlvksWMM0"},"source":["### 1. Install modul Beautifulsoup dan requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ndft83IWO6Z"},"outputs":[],"source":["!pip install requests\n","!pip install BeautifulSoup4"]},{"cell_type":"markdown","metadata":{"id":"PuKHvyvyWQDf"},"source":["### 2. Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqg07YSXWSmF"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"04uKFeyKWaS0"},"source":["### 3. Peroses crawling menggunakan bs4 dan request"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7-71OJ3Web-"},"outputs":[],"source":["#Inisialisasi Link URL\n","url = 'https://www.cnnindonesia.com/indeks'\n","\n","headers={\n","    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n","    }\n","\n","#inisialisasi list untuk memasukkan hasil crawling kedalam list\n","listJudul = []\n","listIsi = []\n","\n","#proses perulangan untuk crawling data\n","for page in range(1,4):\n","  req = requests.get(url+str(page), headers=headers)\n","  soup = BeautifulSoup(req.text, 'html.parser')\n","  items = soup.findAll('article')\n","  for it in items:\n","    try: link = it.find('a')['href']\n","    except : link=''\n","    try: title = it.find('h2', 'title').text \n","    except : title=''\n","    \n","    if title != '':\n","      listJudul.append(title)\n","\n","    if link != '':\n","      req2 = requests.get(str(link), headers=headers)\n","      soup2 = BeautifulSoup(req2.text, 'html.parser')\n","      items2 = soup2.findAll(\"div\", \"detail_text\")\n","      for it2 in items2:\n","        try : berita = it2.find('p').text\n","        except : berita=''\n","\n","        if berita != '':\n","          listIsi.append(berita)\n","\n","\n","df_Judul = pd.DataFrame(listJudul, columns=[\"Judul Berita\"])\n","df_Isi = pd.DataFrame(listIsi, columns=[\"Isi Berita\"])\n","\n","data = pd.concat([df_Judul, df_Isi], axis=1)\n","data = data.loc[0:50]\n","data\n"]},{"cell_type":"markdown","metadata":{"id":"AGi9x50SWjej"},"source":["### 4.Export To CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzyY_7XgWl6T"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/My Drive/prosaindata/CrawlingBerita.csv'\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  data.to_csv(f)"]}]}